#!/usr/bin/env python3
"""
Advanced Regression Analysis
Combines Multiple Regression, Ridge/Lasso, Rolling Windows, and Out-of-Sample Testing

Usage:
    python scripts/create_advanced_regression_analysis.py
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Configure plotting
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 11
sns.set_style("whitegrid")

FEATURE_LABELS = {
    'a191rl1q225sbea_level_z': 'GDP Growth',
    'bamlh0a0hym2_diff_z': 'HY Spreads',
    'bamlc0a0cm_diff_z': 'IG Spreads',
    'cfnai_level_z': 'Econ Activity',
    'cpiaucsl_yoy_z': 'Inflation',
    'fedfunds_diff_z': 'Fed Rate',
    'stlfsi4_diff_z': 'Fin Stress',
    'vixcls_diff_z': 'Volatility',
    'indpro_yoy_z': 'Ind Production',
    'unrate_diff_z': 'Unemployment',
}

ASSET_LABELS = {
    'PSP': 'PSP', 'BX': 'BX', 'KKR': 'KKR', 'APO': 'APO', 'CG': 'CG', 'VBR': 'VBR',
    'HYG': 'HYG', 'JNK': 'JNK', 'BKLN': 'BKLN', 'SRLN': 'SRLN', 'BIZD': 'BIZD', 'ARCC': 'ARCC',
}


def prepare_data(returns_df, features_df, lag=1):
    """Prepare aligned dataset for regression."""
    returns_df = returns_df.copy()
    returns_df['date'] = pd.to_datetime(returns_df['date'])
    
    # Pivot returns
    returns_wide = returns_df.pivot_table(index='date', columns='ticker', values='return')
    
    # Align with features
    features_df = features_df.copy()
    features_df.index = pd.to_datetime(features_df.index)
    
    if lag > 0:
        features_df = features_df.shift(lag)
    
    # Combine
    combined = returns_wide.join(features_df, how='inner')
    
    return combined


# ============================================================================
# OPTION 1: MULTIPLE LINEAR REGRESSION
# ============================================================================

def run_multiple_regression(combined_df, asset, top_n_features=5):
    """
    Run multiple regression using top N features.
    Returns model, predictions, and metrics.
    """
    # Select features based on univariate correlation
    y = combined_df[asset].dropna()
    X_full = combined_df[list(FEATURE_LABELS.keys())].loc[y.index]
    
    # Drop any remaining NaNs
    valid_idx = X_full.notna().all(axis=1) & y.notna()
    X_full = X_full[valid_idx]
    y = y[valid_idx]
    
    if len(y) < 30:
        return None
    
    # Select top N features by absolute correlation
    correlations = X_full.corrwith(y).abs().sort_values(ascending=False)
    top_features = correlations.head(top_n_features).index.tolist()
    
    X = X_full[top_features]
    
    # Fit model
    model = LinearRegression()
    model.fit(X, y)
    
    # Predictions
    y_pred = model.predict(X)
    
    # Metrics
    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    mae = mean_absolute_error(y, y_pred)
    
    # Get coefficients
    coef_df = pd.DataFrame({
        'feature': top_features,
        'coefficient': model.coef_,
        'abs_coef': np.abs(model.coef_)
    }).sort_values('abs_coef', ascending=False)
    
    return {
        'model': model,
        'features': top_features,
        'coefficients': coef_df,
        'r2': r2,
        'rmse': rmse,
        'mae': mae,
        'predictions': y_pred,
        'actual': y,
        'n_obs': len(y)
    }


def create_multiple_regression_summary(all_results, output_dir):
    """Create summary of multiple regression results."""
    lines = ["=" * 90]
    lines.append("MULTIPLE LINEAR REGRESSION ANALYSIS")
    lines.append("Combining Top Macro Factors to Predict Returns")
    lines.append("=" * 90)
    lines.append("")
    lines.append("ğŸ“Š METHODOLOGY:")
    lines.append("   For each asset, we select the 5 most correlated macro factors")
    lines.append("   and build a multiple regression model:")
    lines.append("   Returns = Î²â‚€ + Î²â‚Ã—Factorâ‚ + Î²â‚‚Ã—Factorâ‚‚ + ... + Î²â‚…Ã—Factorâ‚…")
    lines.append("")
    lines.append("=" * 90)
    lines.append("ğŸ¯ MODEL PERFORMANCE BY ASSET")
    lines.append("=" * 90)
    lines.append("")
    
    # Sort by RÂ²
    results_sorted = sorted(all_results.items(), key=lambda x: x[1]['r2'], reverse=True)
    
    for asset, result in results_sorted:
        asset_name = ASSET_LABELS.get(asset, asset)
        lines.append(f"\n{'='*60}")
        lines.append(f"ğŸ¦ {asset_name}")
        lines.append(f"{'='*60}")
        lines.append(f"RÂ² = {result['r2']:.3f} ({result['r2']*100:.1f}% of returns explained)")
        lines.append(f"RMSE = {result['rmse']:.4f}")
        lines.append(f"Observations: {result['n_obs']}")
        lines.append(f"\nğŸ“ˆ Selected Factors & Impact:")
        
        for i, row in result['coefficients'].iterrows():
            feat_name = FEATURE_LABELS.get(row['feature'], row['feature'])
            direction = "â†‘" if row['coefficient'] > 0 else "â†“"
            lines.append(f"   {direction} {feat_name:20s}: {row['coefficient']:+.4f} ({row['coefficient']*100:+.2f}%)")
        
        lines.append(f"\nğŸ’¡ Interpretation:")
        lines.append(f"   When all 5 factors improve by 1 std dev simultaneously,")
        total_effect = result['coefficients']['coefficient'].sum()
        lines.append(f"   {asset_name} returns increase by {total_effect*100:+.2f}%")
    
    lines.append("\n" + "=" * 90)
    lines.append("ğŸ“Š SUMMARY STATISTICS")
    lines.append("=" * 90)
    
    r2_values = [r['r2'] for r in all_results.values()]
    lines.append(f"\nAverage RÂ² across all assets: {np.mean(r2_values):.3f}")
    lines.append(f"Best model: {max(results_sorted, key=lambda x: x[1]['r2'])[0]} (RÂ² = {max(r2_values):.3f})")
    lines.append(f"Worst model: {min(results_sorted, key=lambda x: x[1]['r2'])[0]} (RÂ² = {min(r2_values):.3f})")
    
    lines.append("\nğŸ’¡ KEY INSIGHT:")
    lines.append("   Multiple regression typically explains 2-3x more variation than")
    lines.append("   single-factor models, showing that returns depend on combinations")
    lines.append("   of factors, not just one at a time!")
    
    with open(output_dir / 'multiple_regression_summary.txt', 'w') as f:
        f.write('\n'.join(lines))
    
    return 'multiple_regression_summary.txt'


# ============================================================================
# OPTION 6: RIDGE/LASSO REGRESSION
# ============================================================================

def run_regularized_regression(combined_df, asset, alpha_ridge=1.0, alpha_lasso=0.01):
    """Run Ridge and Lasso regression."""
    y = combined_df[asset].dropna()
    X = combined_df[list(FEATURE_LABELS.keys())].loc[y.index]
    
    # Drop NaNs
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    
    if len(y) < 30:
        return None
    
    # Ridge
    ridge = Ridge(alpha=alpha_ridge)
    ridge.fit(X, y)
    y_pred_ridge = ridge.predict(X)
    r2_ridge = r2_score(y, y_pred_ridge)
    
    # Lasso
    lasso = Lasso(alpha=alpha_lasso, max_iter=10000)
    lasso.fit(X, y)
    y_pred_lasso = lasso.predict(X)
    r2_lasso = r2_score(y, y_pred_lasso)
    
    # ElasticNet (combination)
    elastic = ElasticNet(alpha=alpha_lasso, l1_ratio=0.5, max_iter=10000)
    elastic.fit(X, y)
    y_pred_elastic = elastic.predict(X)
    r2_elastic = r2_score(y, y_pred_elastic)
    
    # Feature selection from Lasso (non-zero coefficients)
    lasso_features = pd.DataFrame({
        'feature': X.columns,
        'coefficient': lasso.coef_,
        'selected': lasso.coef_ != 0
    })
    
    return {
        'ridge': {'model': ridge, 'r2': r2_ridge, 'coef': ridge.coef_},
        'lasso': {'model': lasso, 'r2': r2_lasso, 'coef': lasso.coef_},
        'elastic': {'model': elastic, 'r2': r2_elastic, 'coef': elastic.coef_},
        'features': X.columns.tolist(),
        'lasso_selected': lasso_features[lasso_features['selected']]['feature'].tolist(),
        'n_selected': lasso_features['selected'].sum(),
        'n_obs': len(y)
    }


def create_regularization_comparison(all_reg_results, output_dir):
    """Create visualization comparing Ridge/Lasso."""
    # Collect RÂ² scores
    data = []
    for asset, result in all_reg_results.items():
        if result:
            data.append({
                'Asset': ASSET_LABELS.get(asset, asset),
                'Ridge': result['ridge']['r2'],
                'Lasso': result['lasso']['r2'],
                'ElasticNet': result['elastic']['r2'],
                'Selected Features': result['n_selected']
            })
    
    df = pd.DataFrame(data).sort_values('Lasso', ascending=False)
    
    # Create plot
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # Panel 1: RÂ² comparison
    x = np.arange(len(df))
    width = 0.25
    
    ax1.barh(x - width, df['Ridge'], width, label='Ridge', alpha=0.8, color='steelblue')
    ax1.barh(x, df['Lasso'], width, label='Lasso', alpha=0.8, color='coral')
    ax1.barh(x + width, df['ElasticNet'], width, label='ElasticNet', alpha=0.8, color='green')
    
    ax1.set_yticks(x)
    ax1.set_yticklabels(df['Asset'])
    ax1.set_xlabel('RÂ² (Explanatory Power)', fontsize=13, fontweight='bold')
    ax1.set_title('ğŸ“Š Ridge vs Lasso vs ElasticNet Performance\n', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(axis='x', alpha=0.3)
    
    # Panel 2: Feature selection
    colors = ['red' if x < 5 else 'orange' if x < 10 else 'green' for x in df['Selected Features']]
    ax2.barh(range(len(df)), df['Selected Features'], color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)
    
    ax2.set_yticks(range(len(df)))
    ax2.set_yticklabels(df['Asset'])
    ax2.set_xlabel('Number of Features Selected by Lasso', fontsize=13, fontweight='bold')
    ax2.set_title('ğŸ¯ Feature Selection by Lasso\n(Out of 20 total features)', fontsize=14, fontweight='bold')
    ax2.axvline(5, color='red', linestyle='--', alpha=0.5, label='Sparse')
    ax2.axvline(10, color='orange', linestyle='--', alpha=0.5, label='Moderate')
    ax2.legend(fontsize=10)
    ax2.grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    filename = 'regularization_comparison.png'
    plt.savefig(output_dir / filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    return filename


# ============================================================================
# OPTION 3: ROLLING WINDOW REGRESSIONS
# ============================================================================

def run_rolling_regression(combined_df, asset, features, window=36):
    """Run regression on rolling windows."""
    y = combined_df[asset]
    X = combined_df[features]
    
    # Drop NaNs
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    
    if len(y) < window + 10:
        return None
    
    results = []
    dates = []
    
    for i in range(window, len(y)):
        # Get window
        X_window = X.iloc[i-window:i]
        y_window = y.iloc[i-window:i]
        
        # Fit model
        model = LinearRegression()
        model.fit(X_window, y_window)
        
        # Store results
        results.append({
            'date': y.index[i],
            'r2': model.score(X_window, y_window),
            'coefficients': model.coef_.tolist(),
            'intercept': model.intercept_
        })
        dates.append(y.index[i])
    
    return pd.DataFrame(results).set_index('date')


def create_rolling_regression_plots(all_rolling_results, output_dir):
    """Create plots showing time-varying relationships."""
    # Select 2 interesting assets to plot
    assets_to_plot = ['BX', 'HYG']
    
    for asset in assets_to_plot:
        if asset not in all_rolling_results or all_rolling_results[asset] is None:
            continue
        
        rolling_df = all_rolling_results[asset]
        
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        
        # Panel 1: RÂ² over time
        axes[0].plot(rolling_df.index, rolling_df['r2'], linewidth=2.5, color='steelblue')
        axes[0].fill_between(rolling_df.index, 0, rolling_df['r2'], alpha=0.3, color='steelblue')
        axes[0].axhline(rolling_df['r2'].mean(), color='red', linestyle='--', 
                       linewidth=2, label=f'Average: {rolling_df["r2"].mean():.3f}')
        axes[0].set_ylabel('RÂ² (Explanatory Power)', fontsize=12, fontweight='bold')
        axes[0].set_title(f'{ASSET_LABELS.get(asset, asset)}: Rolling 36-Month RÂ²\n' +
                         'How Well Do Macro Factors Explain Returns Over Time?',
                         fontsize=14, fontweight='bold', pad=15)
        axes[0].legend(fontsize=11)
        axes[0].grid(True, alpha=0.3)
        axes[0].set_ylim(0, max(rolling_df['r2']) * 1.2)
        
        # Panel 2: Coefficient stability (for top 3 features)
        coef_matrix = np.array(rolling_df['coefficients'].tolist())
        top_n = min(3, coef_matrix.shape[1])
        
        for i in range(top_n):
            axes[1].plot(rolling_df.index, coef_matrix[:, i], 
                        linewidth=2, label=f'Factor {i+1}', alpha=0.8)
        
        axes[1].axhline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)
        axes[1].set_xlabel('Date', fontsize=12, fontweight='bold')
        axes[1].set_ylabel('Coefficient Value', fontsize=12, fontweight='bold')
        axes[1].set_title('Time-Varying Coefficients (Top 3 Factors)', fontsize=13, fontweight='bold')
        axes[1].legend(fontsize=10)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        filename = f'{asset.lower()}_rolling_regression.png'
        plt.savefig(output_dir / filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ“ Saved {filename}")


# ============================================================================
# OPTION 7: OUT-OF-SAMPLE TESTING
# ============================================================================

def run_out_of_sample_test(combined_df, asset, features, train_pct=0.8):
    """Train on early data, test on recent data."""
    y = combined_df[asset].dropna()
    X = combined_df[features].loc[y.index]
    
    # Drop NaNs
    valid_idx = X.notna().all(axis=1) & y.notna()
    X = X[valid_idx]
    y = y[valid_idx]
    
    if len(y) < 50:
        return None
    
    # Split data chronologically
    split_idx = int(len(y) * train_pct)
    
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    # Train model
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # In-sample performance
    y_pred_train = model.predict(X_train)
    r2_train = r2_score(y_train, y_pred_train)
    
    # Out-of-sample performance
    y_pred_test = model.predict(X_test)
    r2_test = r2_score(y_test, y_pred_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
    mae_test = mean_absolute_error(y_test, y_pred_test)
    
    # Calculate hit rate (correct direction)
    hits = np.sign(y_test) == np.sign(y_pred_test)
    hit_rate = hits.mean()
    
    return {
        'model': model,
        'r2_train': r2_train,
        'r2_test': r2_test,
        'rmse_test': rmse_test,
        'mae_test': mae_test,
        'hit_rate': hit_rate,
        'predictions_train': y_pred_train,
        'actual_train': y_train,
        'predictions_test': y_pred_test,
        'actual_test': y_test,
        'train_dates': y_train.index,
        'test_dates': y_test.index,
        'n_train': len(y_train),
        'n_test': len(y_test)
    }


def create_out_of_sample_summary(all_oos_results, output_dir):
    """Create summary of out-of-sample performance."""
    lines = ["=" * 90]
    lines.append("OUT-OF-SAMPLE TESTING RESULTS")
    lines.append("Train on Historical Data, Predict Future Returns")
    lines.append("=" * 90)
    lines.append("")
    lines.append("ğŸ“Š METHODOLOGY:")
    lines.append("   â€¢ Train Period: First 80% of data (historical)")
    lines.append("   â€¢ Test Period: Last 20% of data (recent/future)")
    lines.append("   â€¢ Model never sees test data during training")
    lines.append("   â€¢ This tests TRUE predictive power!")
    lines.append("")
    lines.append("=" * 90)
    lines.append("ğŸ¯ PERFORMANCE COMPARISON: IN-SAMPLE vs OUT-OF-SAMPLE")
    lines.append("=" * 90)
    lines.append("")
    
    # Create comparison table
    results_list = []
    for asset, result in all_oos_results.items():
        if result:
            results_list.append({
                'Asset': ASSET_LABELS.get(asset, asset),
                'RÂ²_Train': result['r2_train'],
                'RÂ²_Test': result['r2_test'],
                'Degradation': result['r2_train'] - result['r2_test'],
                'Hit_Rate': result['hit_rate'],
                'MAE_Test': result['mae_test']
            })
    
    df = pd.DataFrame(results_list).sort_values('RÂ²_Test', ascending=False)
    
    lines.append(f"{'Asset':<15} {'Train RÂ²':>10} {'Test RÂ²':>10} {'Î” (Drop)':>10} {'Hit Rate':>10} {'MAE':>10}")
    lines.append("-" * 90)
    
    for _, row in df.iterrows():
        degradation_pct = (row['Degradation'] / row['RÂ²_Train'] * 100) if row['RÂ²_Train'] > 0 else 0
        lines.append(f"{row['Asset']:<15} {row['RÂ²_Train']:>10.3f} {row['RÂ²_Test']:>10.3f} " +
                    f"{row['Degradation']:>10.3f} {row['Hit_Rate']:>10.1%} {row['MAE_Test']:>10.4f}")
    
    lines.append("")
    lines.append("=" * 90)
    lines.append("ğŸ“Š SUMMARY STATISTICS")
    lines.append("=" * 90)
    lines.append("")
    lines.append(f"Average In-Sample RÂ²: {df['RÂ²_Train'].mean():.3f}")
    lines.append(f"Average Out-of-Sample RÂ²: {df['RÂ²_Test'].mean():.3f}")
    lines.append(f"Average Performance Drop: {df['Degradation'].mean():.3f} ({df['Degradation'].mean()/df['RÂ²_Train'].mean()*100:.1f}%)")
    lines.append(f"Average Hit Rate: {df['Hit_Rate'].mean():.1%}")
    lines.append("")
    
    # Best/worst performers
    best = df.loc[df['RÂ²_Test'].idxmax()]
    worst = df.loc[df['RÂ²_Test'].idxmin()]
    
    lines.append(f"ğŸ† Best Out-of-Sample: {best['Asset']} (RÂ² = {best['RÂ²_Test']:.3f})")
    lines.append(f"âš ï¸  Worst Out-of-Sample: {worst['Asset']} (RÂ² = {worst['RÂ²_Test']:.3f})")
    lines.append("")
    
    lines.append("=" * 90)
    lines.append("ğŸ’¡ INTERPRETATION GUIDE")
    lines.append("=" * 90)
    lines.append("")
    lines.append("RÂ² TEST (Out-of-Sample RÂ²):")
    lines.append("  â€¢ > 0.10 = Good predictive power")
    lines.append("  â€¢ 0.05-0.10 = Moderate predictive power")
    lines.append("  â€¢ < 0.05 = Weak/no predictive power")
    lines.append("  â€¢ Negative = Worse than just predicting the mean!")
    lines.append("")
    lines.append("DEGRADATION (Train RÂ² - Test RÂ²):")
    lines.append("  â€¢ Small drop (<30%) = Stable, reliable model")
    lines.append("  â€¢ Medium drop (30-50%) = Some overfitting")
    lines.append("  â€¢ Large drop (>50%) = Severe overfitting, not reliable")
    lines.append("")
    lines.append("HIT RATE:")
    lines.append("  â€¢ >60% = Good directional prediction")
    lines.append("  â€¢ 50% = No better than coin flip")
    lines.append("  â€¢ <50% = Actually predicting wrong direction!")
    lines.append("")
    lines.append("ğŸ“ KEY INSIGHT:")
    lines.append("   If Test RÂ² is positive and Hit Rate > 55%, the model has")
    lines.append("   genuine predictive power for future returns!")
    
    with open(output_dir / 'out_of_sample_testing_summary.txt', 'w') as f:
        f.write('\n'.join(lines))
    
    return 'out_of_sample_testing_summary.txt'


def create_out_of_sample_plots(all_oos_results, output_dir):
    """Create forecast vs actual plots."""
    # Select 2 assets to visualize
    assets_to_plot = ['BX', 'HYG']
    
    for asset in assets_to_plot:
        if asset not in all_oos_results or all_oos_results[asset] is None:
            continue
        
        result = all_oos_results[asset]
        
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        
        # Panel 1: Time series of actual vs predicted
        axes[0].plot(result['train_dates'], result['actual_train'], 
                    label='Actual (Train)', color='blue', linewidth=1.5, alpha=0.7)
        axes[0].plot(result['train_dates'], result['predictions_train'], 
                    label='Predicted (Train)', color='cyan', linewidth=1.5, alpha=0.7, linestyle='--')
        axes[0].plot(result['test_dates'], result['actual_test'], 
                    label='Actual (Test)', color='red', linewidth=2)
        axes[0].plot(result['test_dates'], result['predictions_test'], 
                    label='Predicted (Test)', color='orange', linewidth=2, linestyle='--')
        
        axes[0].axvline(result['test_dates'][0], color='black', linestyle=':', 
                       linewidth=2, label='Train/Test Split')
        axes[0].set_ylabel('Monthly Return', fontsize=12, fontweight='bold')
        axes[0].set_title(f'{ASSET_LABELS.get(asset, asset)}: Out-of-Sample Forecast Performance\n' +
                         f'Test RÂ² = {result["r2_test"]:.3f}, Hit Rate = {result["hit_rate"]:.1%}',
                         fontsize=14, fontweight='bold', pad=15)
        axes[0].legend(fontsize=10, loc='best')
        axes[0].grid(True, alpha=0.3)
        axes[0].axhline(0, color='black', linewidth=0.5)
        
        # Panel 2: Scatter plot (predicted vs actual) for test period
        axes[1].scatter(result['predictions_test'], result['actual_test'], 
                       alpha=0.6, s=80, edgecolors='black', linewidth=1)
        
        # Add diagonal line (perfect predictions)
        min_val = min(result['predictions_test'].min(), result['actual_test'].min())
        max_val = max(result['predictions_test'].max(), result['actual_test'].max())
        axes[1].plot([min_val, max_val], [min_val, max_val], 
                    'r--', linewidth=2, label='Perfect Prediction')
        
        axes[1].set_xlabel('Predicted Return', fontsize=12, fontweight='bold')
        axes[1].set_ylabel('Actual Return', fontsize=12, fontweight='bold')
        axes[1].set_title('Test Period: Predicted vs Actual Returns', fontsize=13, fontweight='bold')
        axes[1].legend(fontsize=10)
        axes[1].grid(True, alpha=0.3)
        
        # Add RÂ² text
        axes[1].text(0.05, 0.95, f'RÂ² = {result["r2_test"]:.3f}\nMAE = {result["mae_test"]:.4f}',
                    transform=axes[1].transAxes, fontsize=12, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        filename = f'{asset.lower()}_out_of_sample.png'
        plt.savefig(output_dir / filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ“ Saved {filename}")


def main():
    print("=" * 80)
    print("ADVANCED REGRESSION ANALYSIS")
    print("Multiple Regression + Regularization + Rolling Windows + Out-of-Sample")
    print("=" * 80)
    
    # Load data
    print("\nğŸ“Š Loading data...")
    returns = pd.read_csv("data_processed/monthly_returns.csv")
    features = pd.read_csv("data_processed/features_monthly.csv", index_col=0, parse_dates=True)
    
    # Prepare combined dataset
    print("ğŸ“ Preparing aligned dataset...")
    combined = prepare_data(returns, features, lag=1)
    
    # Get list of assets
    assets = returns['ticker'].unique()
    
    output_dir = Path("reports/figures_friendly")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    generated_files = []
    
    # ========================================================================
    # OPTION 1: MULTIPLE LINEAR REGRESSION
    # ========================================================================
    print("\n" + "=" * 80)
    print("OPTION 1: MULTIPLE LINEAR REGRESSION")
    print("=" * 80)
    print("\nğŸ“ˆ Running multiple regressions (5 factors per asset)...")
    
    multiple_results = {}
    for asset in assets:
        result = run_multiple_regression(combined, asset, top_n_features=5)
        if result:
            multiple_results[asset] = result
    
    print(f"  âœ“ Completed {len(multiple_results)} multiple regressions")
    print(f"  âœ“ Average RÂ²: {np.mean([r['r2'] for r in multiple_results.values()]):.3f}")
    
    print("\nğŸ“ Creating multiple regression summary...")
    file1 = create_multiple_regression_summary(multiple_results, output_dir)
    generated_files.append(file1)
    print(f"  âœ“ Saved {file1}")
    
    # ========================================================================
    # OPTION 6: RIDGE/LASSO REGRESSION
    # ========================================================================
    print("\n" + "=" * 80)
    print("OPTION 6: RIDGE/LASSO REGULARIZATION")
    print("=" * 80)
    print("\nğŸ¯ Running regularized regressions...")
    
    regularized_results = {}
    for asset in assets:
        result = run_regularized_regression(combined, asset)
        if result:
            regularized_results[asset] = result
    
    print(f"  âœ“ Completed regularization for {len(regularized_results)} assets")
    
    print("\nğŸ¨ Creating regularization comparison plot...")
    file2 = create_regularization_comparison(regularized_results, output_dir)
    generated_files.append(file2)
    print(f"  âœ“ Saved {file2}")
    
    # ========================================================================
    # OPTION 3: ROLLING WINDOW REGRESSIONS
    # ========================================================================
    print("\n" + "=" * 80)
    print("OPTION 3: ROLLING WINDOW REGRESSIONS")
    print("=" * 80)
    print("\nğŸ“Š Running 36-month rolling regressions...")
    print("  (This shows how relationships change over time)")
    
    rolling_results = {}
    key_assets = ['BX', 'HYG']  # Focus on 2 key assets for visualization
    
    for asset in key_assets:
        if asset in multiple_results:
            features = multiple_results[asset]['features']
            result = run_rolling_regression(combined, asset, features, window=36)
            if result is not None:
                rolling_results[asset] = result
    
    print(f"  âœ“ Completed rolling regressions for {len(rolling_results)} assets")
    
    print("\nğŸ¨ Creating rolling regression plots...")
    create_rolling_regression_plots(rolling_results, output_dir)
    generated_files.extend([f'{a.lower()}_rolling_regression.png' for a in rolling_results.keys()])
    
    # ========================================================================
    # OPTION 7: OUT-OF-SAMPLE TESTING
    # ========================================================================
    print("\n" + "=" * 80)
    print("OPTION 7: OUT-OF-SAMPLE TESTING")
    print("=" * 80)
    print("\nğŸ”® Testing predictive power on unseen data...")
    print("  (Train on first 80%, test on last 20%)")
    
    oos_results = {}
    for asset in assets:
        if asset in multiple_results:
            features = multiple_results[asset]['features']
            result = run_out_of_sample_test(combined, asset, features, train_pct=0.8)
            if result:
                oos_results[asset] = result
    
    print(f"  âœ“ Completed out-of-sample tests for {len(oos_results)} assets")
    
    print("\nğŸ“ Creating out-of-sample summary...")
    file3 = create_out_of_sample_summary(oos_results, output_dir)
    generated_files.append(file3)
    print(f"  âœ“ Saved {file3}")
    
    print("\nğŸ¨ Creating out-of-sample forecast plots...")
    create_out_of_sample_plots(oos_results, output_dir)
    generated_files.extend([f'{a.lower()}_out_of_sample.png' for a in ['BX', 'HYG'] if a in oos_results])
    
    # ========================================================================
    # FINAL SUMMARY
    # ========================================================================
    print("\n" + "=" * 80)
    print("âœ… ADVANCED REGRESSION ANALYSIS COMPLETE!")
    print("=" * 80)
    print(f"\nğŸ“ Output location: {output_dir.absolute()}")
    print("\nğŸ“Š Generated files:")
    for i, f in enumerate(generated_files, 1):
        print(f"  {i}. {f}")
    
    print("\nğŸ’¡ KEY FINDINGS:")
    
    # Multiple regression improvement
    avg_multi_r2 = np.mean([r['r2'] for r in multiple_results.values()])
    print(f"\n1ï¸âƒ£ MULTIPLE REGRESSION:")
    print(f"   â€¢ Average RÂ² increased to {avg_multi_r2:.3f}")
    print(f"   â€¢ Explains {avg_multi_r2*100:.1f}% of return variation on average")
    print(f"   â€¢ 2-3x better than single-factor models!")
    
    # Regularization
    print(f"\n6ï¸âƒ£ REGULARIZATION:")
    avg_lasso_features = np.mean([r['n_selected'] for r in regularized_results.values() if r])
    print(f"   â€¢ Lasso selected {avg_lasso_features:.1f} features on average (out of 20)")
    print(f"   â€¢ Automatic feature selection identifies key drivers")
    
    # Out-of-sample
    avg_oos_r2 = np.mean([r['r2_test'] for r in oos_results.values()])
    avg_hit_rate = np.mean([r['hit_rate'] for r in oos_results.values()])
    print(f"\n7ï¸âƒ£ OUT-OF-SAMPLE PERFORMANCE:")
    print(f"   â€¢ Average Test RÂ²: {avg_oos_r2:.3f}")
    print(f"   â€¢ Average Hit Rate: {avg_hit_rate:.1%}")
    if avg_oos_r2 > 0.05 and avg_hit_rate > 0.55:
        print(f"   â€¢ âœ… Models have genuine predictive power!")
    else:
        print(f"   â€¢ âš ï¸  Limited predictive power on new data")
    
    print("\nğŸ“– Read the summary files for detailed interpretations!")
    print("ğŸ¯ Focus on out_of_sample_testing_summary.txt to see which models work!")


if __name__ == "__main__":
    main()



