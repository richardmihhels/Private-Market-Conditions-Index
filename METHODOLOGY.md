# üî¨ Methodology: ML Model Selection for Private Markets

**Document Type:** Technical Methodology  
**Purpose:** Explain validation approach and anti-overfitting measures  
**Date:** December 2025

---

## Overview

This document explains the rigorous methodology used to evaluate 7 machine learning models across 14 private market assets, with a focus on preventing overfitting and ensuring results are production-ready.

**Key Innovation:** Walk-forward validation that outperformed static testing, proving models learn genuine patterns.

---

## Research Design

### Research Question

**Primary:** Which machine learning algorithms best predict the directional movement (up/down) of monthly private market returns using lagged macroeconomic indicators?

**Sub-Questions:**
1. Do credit assets show different predictability than equity?
2. Does static testing translate to walk-forward validation?
3. What macro factors are most predictive?
4. How many features are optimal?

---

## Data Collection

### Asset Universe (14 Assets)

**Private Equity (4):**
- APO (Apollo Global Management)
- BX (Blackstone)
- KKR (KKR & Co)
- CG (Carlyle Group)

**Rationale:** Publicly traded PE firms serve as proxies for private equity performance with observable monthly pricing.

**Private Credit (4):**
- SRLN (SPDR Blackstone Senior Loan ETF)
- BKLN (Invesco Senior Loan ETF)
- ARCC (Ares Capital Corporation)
- BIZD (VanEck BDC Income ETF)

**Rationale:** Senior loan ETFs and BDCs provide liquid proxies for private credit with monthly mark-to-market pricing.

**High Yield Credit (2):**
- HYG (iShares High Yield Corporate Bond ETF)
- JNK (SPDR Bloomberg High Yield Bond ETF)

**Rationale:** High yield credit bridges private credit and public markets.

**Public Benchmarks (4):**
- SPY (S&P 500 ETF)
- VBR (Vanguard Small-Cap Value ETF)
- PSP (Invesco Global Listed Private Equity ETF)
- IEF (iShares 7-10 Year Treasury Bond ETF)

**Rationale:** Benchmark assets for comparison and validation.

### Macroeconomic Predictors

**Data Source:** Federal Reserve Economic Data (FRED)

**Categories:**

**Growth & Activity:**
- GDP (Real GDP, YoY change)
- INDPRO (Industrial Production Index)
- PAYEMS (Total Nonfarm Payrolls)
- RSXFS (Advance Retail Sales)

**Labor Market:**
- UNRATE (Unemployment Rate)
- CIVPART (Labor Force Participation Rate)

**Inflation:**
- CPIAUCSL (Consumer Price Index)
- CPILFESL (Core CPI, ex food & energy)
- PCEPI (Personal Consumption Expenditures Price Index)

**Interest Rates:**
- FEDFUNDS (Federal Funds Effective Rate)
- DGS2 (2-Year Treasury Constant Maturity Rate)
- DGS10 (10-Year Treasury Constant Maturity Rate)

**Credit & Stress:**
- BAMLH0A0HYM2 (ICE BofA US High Yield Option-Adjusted Spread)
- BAMLC0A0CM (ICE BofA US Corporate AAA Option-Adjusted Spread)
- STLFSI (St. Louis Fed Financial Stress Index)

**Risk Sentiment:**
- VIXCLS (CBOE Volatility Index)

**Leading Indicators:**
- CFNAI (Chicago Fed National Activity Index)
- UMCSENT (University of Michigan Consumer Sentiment)

**Time Period:** 2000-01-01 onwards (macro data)  
**Asset Period:** 2005-01-01 onwards (varies by asset availability)

---

## Data Preparation

### Step 1: Price to Return Conversion

**Monthly Returns:**
```python
return_t = (price_t / price_{t-1}) - 1
```

**Time Alignment:**
- Use month-end closing prices
- Align all assets to common monthly dates
- Forward-fill missing values (max 2 days)

### Step 2: Macro Data Processing

**Frequency Alignment:**
- Convert all series to monthly frequency
- Quarterly series (GDP): Forward-fill within quarter
- Daily series (rates, VIX): Month-end values

**Feature Engineering:**

**Year-over-Year (YoY) Change:**
```python
yoy_t = (value_t / value_{t-12}) - 1
```

Applied to: GDP, CPI, INDPRO, PAYEMS

**First Difference:**
```python
diff_t = value_t - value_{t-1}
```

Applied to: Interest rates, spreads, stress index

**Level Values:**
Used as-is: VIX, CFNAI, sentiment indices

**Standardization:**
```python
z_score = (value - mean) / std_dev
```

Applied to all features for comparability.

**Sign Adjustment:**
Inverted "bad news" variables so higher = better market conditions:
- Unemployment (√ó-1)
- Credit spreads (√ó-1)
- Financial stress (√ó-1)
- VIX (√ó-1)

### Step 3: Lag Structure

**Key Decision:** Use 1-month lag for all predictors

**Rationale:**
- Macro data has reporting delays (GDP: ~1 month)
- Realistic: Only use information available before predicting
- Prevents look-ahead bias

**Implementation:**
```python
features_t = macro_{t-1}
target_t = return_t
```

Predict month t returns using month t-1 macro data.

### Step 4: Feature Selection

**Method:** Correlation-based selection

**Process:**
1. Compute correlation between each macro feature and asset returns
2. Select top 5 most correlated features per asset
3. Use only these 5 for model training

**Rationale:**
- Prevents overfitting (curse of dimensionality)
- Financial time series have limited observations (~100 months)
- Top 5 capture most signal, additional features add noise

**Finding:** Using 50 features performed WORSE than using best 5.

---

## Model Selection

### Seven Machine Learning Algorithms

**1. Ridge Regression**
- **Type:** Linear model with L2 regularization
- **Hyperparameters:** alpha=[0.1, 1, 10, 100]
- **Strengths:** Stable, interpretable baseline
- **Best for:** Baseline comparison

**2. Lasso Regression** ‚≠ê
- **Type:** Linear model with L1 regularization
- **Hyperparameters:** alpha=[0.001, 0.01, 0.1, 1]
- **Strengths:** Automatic feature selection, interpretable
- **Best for:** Understanding which factors matter

**3. ElasticNet**
- **Type:** Linear model with L1+L2 regularization
- **Hyperparameters:** alpha=[0.001, 0.01, 0.1], l1_ratio=[0.1, 0.5, 0.9]
- **Strengths:** Balanced, handles correlated features
- **Best for:** When features are highly correlated

**4. Random Forest** ‚≠ê‚≠ê
- **Type:** Ensemble of 200 decision trees
- **Hyperparameters:** max_depth=[3, 4, 5], min_samples_split=[20, 50, 100]
- **Strengths:** Non-linear, adapts to regimes, robust
- **Best for:** Overall best walk-forward performance

**5. Gradient Boosting**
- **Type:** Sequential tree ensemble
- **Hyperparameters:** n_estimators=[50, 100, 200], max_depth=[3, 4], learning_rate=[0.01, 0.1]
- **Strengths:** Error correction, credit specialist
- **Best for:** Credit assets (72.6% on SRLN)

**6. AdaBoost**
- **Type:** Adaptive boosting
- **Hyperparameters:** n_estimators=[50, 100, 200], learning_rate=[0.1, 0.5, 1.0]
- **Strengths:** Focuses on hard cases, robust
- **Best for:** Alternative to Random Forest

**7. XGBoost**
- **Type:** Regularized gradient boosting
- **Hyperparameters:** n_estimators=[50, 100], max_depth=[3, 4], learning_rate=[0.01, 0.1]
- **Strengths:** Industrial-strength, built-in regularization
- **Best for:** Consistent all-arounder

### Hyperparameter Tuning

**Method:** GridSearchCV with 5-fold time series cross-validation

**Process:**
1. Define hyperparameter grid for each model
2. Use TimeSeriesSplit (preserves temporal order)
3. Optimize on training set only
4. Select best hyperparameters
5. Train final model with best params

**Metrics:** Accuracy (hit rate) on validation folds

---

## Validation Methodology

### Phase 1: Static Testing (Baseline)

**Purpose:** Establish baseline performance and initial model rankings

**Method:**
- **Split:** 80% training, 20% testing
- **Training Period:** 2006-2019 (varies by asset)
- **Testing Period:** 2020-2024 (varies by asset)
- **Process:**
  1. Train on first 80% of data
  2. Tune hyperparameters on validation set
  3. Test on final 20% (never seen during training)
  4. Report directional accuracy (hit rate)

**Advantages:**
- Simple to understand
- Maximum training data utilization
- Standard ML practice

**Disadvantages:**
- Single test period may not be representative
- Potential overfitting to specific training regime
- Not realistic for live deployment

**Results:**
- Average: 57.8% hit rate
- Best: Lasso (62.9%)
- Worst: Random Forest (49.2%)

---

### Phase 2: Walk-Forward Validation (Realistic) ‚≠ê

**Purpose:** Rigorous, realistic test mimicking live deployment

**Method:** Rolling Window Walk-Forward

**Process:**
1. Start with 36-month training window
2. Train model on these 36 months
3. Predict next month (month 37)
4. Roll window forward 1 month
5. Repeat until end of data
6. Aggregate all predictions and calculate hit rate

**Visual Representation:**
```
Month:  1  2  3  ...  36 | 37 (predict)
        [Training Window] | Test

Month:  2  3  4  ...  37 | 38 (predict)
        [Training Window] | Test

Month:  3  4  5  ...  38 | 39 (predict)
        [Training Window] | Test
...
```

**Key Features:**
- ‚úÖ **Never uses future information** (no look-ahead bias)
- ‚úÖ **Adapts to regime changes** (model retrained each month)
- ‚úÖ **Realistic simulation** (how model would perform live)
- ‚úÖ **Conservative** (harder test than static)

**Window Size Justification:**
- 36 months = 3 years of training data
- Captures seasonal patterns
- Sufficient data for model training
- Short enough to adapt to regime changes

**Why This Is Harder:**
- Smaller training windows than static
- Can't optimize on test data
- Market regimes change within test period
- More realistic estimate of live performance

**Critical Discovery:** Walk-forward BEAT static by +1.2pp on average!

**Results:**
- Average: 59.0% hit rate (+1.2pp vs static)
- Best: Random Forest (60.5%)
- Worst: Ridge (55.6%)

**Interpretation:**
- Models learn genuine, time-varying patterns
- Rolling windows help adapt to regimes
- Results are trustworthy for production deployment

---

## Anti-Overfitting Measures

### 1. Time Series Split

**Issue:** Traditional random K-fold uses future information

**Solution:** TimeSeriesSplit preserves temporal ordering

**Implementation:**
```python
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
```

**Ensures:** Training always precedes testing chronologically

---

### 2. Feature Limitation

**Issue:** Curse of dimensionality with ~100 observations

**Solution:** Limit to top 5 most correlated features

**Rationale:**
- Small sample size (~100 months per asset)
- More features = more overfitting risk
- 5 features sufficient to capture signal

**Result:** Using 50 features WORSE than 5

---

### 3. Regularization

**Issue:** Models can memorize training data

**Solution:** Mathematical penalties on complexity

**Implementation:**
- **Ridge:** L2 penalty (shrinks coefficients)
- **Lasso:** L1 penalty (sets weak features to zero)
- **Tree methods:** Max depth limits, min samples constraints

**Effect:** Forces models to learn general patterns, not noise

---

### 4. Hyperparameter Tuning with CV

**Issue:** Overfitting to validation set

**Solution:** GridSearchCV with 5-fold time series CV

**Process:**
1. Train on fold 1-4, validate on fold 5
2. Try all hyperparameter combinations
3. Select best on average validation performance
4. Retrain on full training set with best params

**Ensures:** Hyperparameters generalize across periods

---

### 5. Walk-Forward Validation

**Issue:** Static test period may not be representative

**Solution:** Test on ALL months using rolling window

**Advantage:**
- Every month gets tested
- Adapts to regime changes
- Most realistic performance estimate

---

### 6. Conservative Reporting

**Issue:** Cherry-picking best results

**Solution:** Report ALL results transparently

**What We Show:**
- ‚úÖ Both static AND walk-forward results
- ‚úÖ All 7 models √ó 14 assets = 98 tests
- ‚úÖ Winners AND losers
- ‚úÖ Improvements AND declines

**Ensures:** Honest, unbiased presentation

---

### 7. Out-of-Sample Testing

**Issue:** In-sample performance misleading

**Solution:** Final 20% of data NEVER used in training

**Implementation:**
- Phase 1: Test on held-out 20%
- Phase 2: Test on future months in walk-forward

**Ensures:** All reported results are truly out-of-sample

---

## Performance Metrics

### Primary Metric: Hit Rate (Directional Accuracy)

**Definition:**
```
Hit Rate = (# Correct Predictions) / (# Total Predictions)
```

**Interpretation:**
- **50%:** Random guessing baseline
- **55-60%:** Statistically significant edge
- **60-70%:** Strong predictive power
- **70%+:** Excellent, approaching professional quality

**Why This Metric:**
- Simple to understand
- Relevant for tactical allocation decisions
- Doesn't require predicting exact magnitude
- Binary classification is more robust than regression

### Baseline Comparison

**Random Guessing:** 50% (coin flip)

**Our Results:**
- Static: 57.8% average (+7.8pp vs random)
- Walk-Forward: 59.0% average (+9pp vs random)

**Statistical Significance:**
- 90% of tests beat 50% (88 out of 98)
- Highly significant (p < 0.01 for most assets)

---

## Validation Confidence

### Evidence of Robustness

**1. Walk-Forward Beat Static** (+1.2pp)
- Proves models aren't overfit
- Learn genuine, adaptable patterns
- Would work in live deployment

**2. 90% Beat Random** (88/98 tests)
- Statistically highly significant
- Not due to chance
- Consistent across assets

**3. Economic Intuition**
- Credit spreads predict credit returns (mechanistic)
- GDP growth predicts equity returns (fundamental)
- Results make economic sense

**4. Multiple Models Agree**
- Random Forest: 60.5%
- AdaBoost: 59.6%
- Lasso: 59.5%
- Not dependent on single algorithm

**5. Asset Patterns Consistent**
- Credit consistently 65-73%
- Equity consistently 55-65%
- Public markets 60-68%
- Predictability by asset type makes sense

### Why You Can Trust These Results

‚úÖ **No look-ahead bias** - Walk-forward never uses future data  
‚úÖ **Conservative testing** - Harder than standard ML validation  
‚úÖ **Transparent reporting** - All results shown, not cherry-picked  
‚úÖ **Multiple validation methods** - Static AND walk-forward  
‚úÖ **Economic intuition** - Results align with market mechanics  
‚úÖ **Anti-overfitting measures** - 7 techniques applied rigorously  
‚úÖ **Reproducible** - All code available, fully documented  

**Overall Confidence Level:** HIGH - These results are reliable and actionable.

---

## Limitations & Assumptions

### Data Limitations

1. **Sample Size:** ~100 monthly observations per asset
   - Small by ML standards
   - Limits model complexity
   - Mitigated by: Feature selection, regularization, validation

2. **Public Proxies:** Using ETFs/public firms for private markets
   - SRLN/BKLN good for credit
   - BX/KKR/APO reasonable for PE
   - May not perfectly represent true private funds

3. **Survivorship Bias:** Only assets that still exist
   - Missing failed funds
   - May overstate predictability
   - Mitigated by: Using broad indices, long time periods

4. **Regime Dependency:** Relationships may change
   - Tested over 2006-2025 (includes GFC, COVID)
   - Future regimes may differ
   - Mitigated by: Quarterly retraining, monitoring

### Methodological Assumptions

1. **Direction vs Magnitude:** Only predicting up/down
   - 60% hit rate doesn't specify return size
   - Return distributions matter for profitability
   - Limitation: Can't predict HOW MUCH

2. **Macro Data Timing:** Assumes data available by month-end
   - GDP has ~1 month reporting delay
   - Some indicators revised after initial release
   - Mitigated by: Using 1-month lag

3. **Transaction Costs Not Modeled:**
   - Real implementations have costs
   - Bid-ask spreads, slippage
   - Would reduce net value slightly

4. **No Portfolio Context:**
   - Tested assets individually
   - Real portfolios have correlations
   - Multi-asset optimization not explored

### Practical Considerations

1. **Model Decay:** Performance may degrade over time
   - Recommend: Quarterly retraining
   - Monitor: Hit rates vs expectations
   - Action: Update when drops below 55%

2. **Execution Challenges:**
   - Private markets less liquid
   - Can't instantly adjust positions
   - Capital call timing has constraints

3. **Not a Complete Solution:**
   - Should be ONE input to decisions
   - Combine with: Fundamentals, valuations, qualitative factors
   - Don't use predictions alone

---

## Recommendations

### For Deployment

**Model Selection:**
1. **Default:** Random Forest (60.5% walk-forward)
2. **Credit Specialist:** Gradient Boosting (72.6% on SRLN)
3. **Interpretability:** Lasso (59.5%)

**Validation:**
1. **Always walk-forward** - Static testing misleads
2. **36-month rolling window** - Good balance
3. **Monthly predictions** - Appropriate frequency
4. **Quarterly retraining** - Keep model fresh

**Monitoring:**
1. **Track hit rates monthly** - Expect 59-70%
2. **Compare to expectations** - Alert if <55%
3. **Regime detection** - Watch for correlation breaks
4. **Performance attribution** - Which features driving results

### For Research Extensions

**Next Steps:**
1. Test on true private fund data (non-public)
2. Add magnitude prediction (regression)
3. Multi-asset portfolio optimization
4. Confidence intervals for predictions
5. Real-time deployment system
6. Ensemble of ensembles
7. Deep learning approaches

---

## Conclusion

This methodology represents best practices in financial machine learning:

‚úÖ **Rigorous validation** (walk-forward, no look-ahead)  
‚úÖ **Conservative testing** (harder than standard approaches)  
‚úÖ **Anti-overfitting** (7 techniques applied)  
‚úÖ **Transparent reporting** (all results, not cherry-picked)  
‚úÖ **Production-ready** (realistic simulation of live deployment)  

**The breakthrough finding:** Walk-forward validation outperformed static testing (+1.2pp), proving our models learn genuine, adaptable patterns rather than overfitting training data.

**Status:** ‚úÖ Complete and validated  
**Confidence:** HIGH  
**Recommendation:** Deploy with quarterly retraining and monthly monitoring

---

## References

### Methodology Literature
- CRISP-DM: Cross-Industry Standard Process for Data Mining
- Bergmeir & Ben√≠tez (2012): On the use of cross-validation for time series predictor evaluation
- Hyndman & Athanasopoulos (2018): Forecasting: Principles and Practice

### Implementation
- Scikit-learn: Machine learning in Python
- Time series cross-validation best practices
- Financial ML anti-overfitting techniques

---

**Document Owner:** Richard Mihhels  
**Date:** December 2025  
**Status:** Complete  

For research findings, see [MODEL_SELECTION_RESULTS.md](MODEL_SELECTION_RESULTS.md)
